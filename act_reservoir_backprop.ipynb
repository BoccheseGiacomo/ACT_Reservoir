{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACT_reservoir:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, max_iter=20, learning_rate=0.01, ponder_cost=0.01):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ponder_cost = ponder_cost\n",
    "\n",
    "        self.total_dim = 1 + input_dim + hidden_dim + output_dim + 1  # Including bias and halting node\n",
    "        \n",
    "        # Initializing the weight matrix with random values between -2 and 2 and setting requires_grad=True\n",
    "        self.W = (torch.rand((self.total_dim, self.total_dim)) * 4 - 2).clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # Index ranges for different parts of the state vector\n",
    "        self.input_indices = range(1, input_dim + 1)\n",
    "        self.output_indices = range(input_dim + hidden_dim + 1, input_dim + hidden_dim + output_dim + 1)\n",
    "        self.halting_index = self.total_dim - 1  # Last element as the halting unit\n",
    "\n",
    "        # Initializing Adam optimizer\n",
    "        self.optimizer = optim.Adam([self.W], lr=learning_rate)\n",
    "        \n",
    "    def predict(self, xi):\n",
    "        x = torch.zeros((self.total_dim, 1), requires_grad=True)\n",
    "        x_new = x.clone()  # Creating a new tensor by cloning the original tensor\n",
    "        x_new[0] = 1  # Bias unit\n",
    "        x_new[self.input_indices] = torch.Tensor(xi).view((self.input_dim, 1))  # Input part\n",
    "        self.x = x_new  # Replacing the old tensor with the new one\n",
    "\n",
    "        t = 0\n",
    "        while t < self.max_iter and self.x[self.halting_index].item() <= 1:\n",
    "            self.x = self.W @ self.x #the core of the network\n",
    "            #self.x=F.relu(self.x) #enable to add non-linearities\n",
    "            self.x.retain_grad()  # Retain gradients at each step for BPTT\n",
    "            t += 1\n",
    "        return self.x[self.output_indices], t\n",
    "    \n",
    "    def train(self, xi, target):\n",
    "        self.optimizer.zero_grad()  # Clear old gradients\n",
    "        predicted_output, steps = self.predict(xi)\n",
    "        error = predicted_output - torch.Tensor(target).view((-1, 1))\n",
    "\n",
    "        # Calculating the MSE loss and adding the ponder cost\n",
    "        loss = torch.mean(error ** 2) + self.ponder_cost * steps\n",
    "        loss.backward()  # Compute the gradients\n",
    "        \n",
    "        # Update the weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"Total Parameters:\", self.total_dim ** 2)\n",
    "        print(\"Input Dimension:\", self.input_dim)\n",
    "        print(\"Hidden Dimension:\", self.hidden_dim)\n",
    "        print(\"Output Dimension:\", self.output_dim)\n",
    "        print(\"Max Iterations:\", self.max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 21879.714569593743\n",
      "Epoch 10, Loss: 19922.51134843439\n",
      "Epoch 20, Loss: 18366.42053228013\n",
      "Epoch 30, Loss: 16899.67541829373\n",
      "Epoch 40, Loss: 15523.320453680064\n",
      "Epoch 50, Loss: 14236.006366672535\n",
      "Epoch 60, Loss: 13036.220109713515\n",
      "Epoch 70, Loss: 11922.23675569729\n",
      "Epoch 80, Loss: 10892.17760000402\n",
      "Epoch 90, Loss: 9943.987055785507\n",
      "Epoch 100, Loss: 9075.547996951593\n",
      "Epoch 110, Loss: 8284.077886840701\n",
      "Epoch 120, Loss: 7566.939908095701\n",
      "Epoch 130, Loss: 6921.105356479697\n",
      "Epoch 140, Loss: 6343.268455157476\n",
      "Epoch 150, Loss: 5829.9090175028705\n",
      "Epoch 160, Loss: 5377.189117958471\n",
      "Epoch 170, Loss: 4981.060699903779\n",
      "Epoch 180, Loss: 4637.260159416636\n",
      "Epoch 190, Loss: 4341.359433137421\n",
      "Epoch 200, Loss: 4088.8223329504394\n",
      "Epoch 210, Loss: 3875.1325167902\n",
      "Epoch 220, Loss: 3695.799106949288\n",
      "Epoch 230, Loss: 3546.5291930224093\n",
      "Epoch 240, Loss: 3423.1991494665667\n",
      "Epoch 250, Loss: 3322.019689152492\n",
      "Epoch 260, Loss: 3239.5063349727634\n",
      "Epoch 270, Loss: 3172.5470114914046\n",
      "Epoch 280, Loss: 3118.424866281897\n",
      "Epoch 290, Loss: 3074.784617586015\n",
      "Epoch 300, Loss: 3039.6301968104763\n",
      "Epoch 310, Loss: 3011.2741079991406\n",
      "Epoch 320, Loss: 2988.3358128119726\n",
      "Epoch 330, Loss: 2969.686622195095\n",
      "Epoch 340, Loss: 2954.4068053553997\n",
      "Epoch 350, Loss: 2941.7627251819895\n",
      "Epoch 360, Loss: 2931.174231420206\n",
      "Epoch 370, Loss: 2922.1747884136903\n",
      "Epoch 380, Loss: 2914.4053518950286\n",
      "Epoch 390, Loss: 2907.5803812270983\n",
      "Epoch 400, Loss: 2901.488050253503\n",
      "Epoch 410, Loss: 2895.952224497534\n",
      "Epoch 420, Loss: 2890.8458739578723\n",
      "Epoch 430, Loss: 2886.0665484187007\n",
      "Epoch 440, Loss: 2881.5412487880885\n",
      "Epoch 450, Loss: 2877.206722266078\n",
      "Epoch 460, Loss: 2873.0238445730506\n",
      "Epoch 470, Loss: 2868.951121707484\n",
      "Epoch 480, Loss: 2864.9728174540587\n",
      "Epoch 490, Loss: 2861.0655514116397\n",
      "Epoch 500, Loss: 2857.2141437813734\n",
      "Epoch 510, Loss: 2853.412637627637\n",
      "Epoch 520, Loss: 2849.6421927974\n",
      "Epoch 530, Loss: 2845.902810028754\n",
      "Epoch 540, Loss: 2842.192643854823\n",
      "Epoch 550, Loss: 2838.506330417413\n",
      "Epoch 560, Loss: 2834.84304455325\n",
      "Epoch 570, Loss: 2831.1992259888725\n",
      "Epoch 580, Loss: 2827.572129129376\n",
      "Epoch 590, Loss: 2823.960752008073\n",
      "Epoch 600, Loss: 2820.3668963050936\n",
      "Epoch 610, Loss: 2816.786753464341\n",
      "Epoch 620, Loss: 2813.219913370535\n",
      "Epoch 630, Loss: 2809.668684492316\n",
      "Epoch 640, Loss: 2806.1321731588523\n",
      "Epoch 650, Loss: 2802.6076759320777\n",
      "Epoch 660, Loss: 2799.0978268081135\n",
      "Epoch 670, Loss: 2795.6018305986563\n",
      "Epoch 680, Loss: 2792.1171352243146\n",
      "Epoch 690, Loss: 2788.645603610985\n",
      "Epoch 700, Loss: 2785.1866775380354\n",
      "Epoch 710, Loss: 2781.7416098043786\n",
      "Epoch 720, Loss: 2778.3064553842787\n",
      "Epoch 730, Loss: 2774.8840207772887\n",
      "Epoch 740, Loss: 2771.474909402598\n",
      "Epoch 750, Loss: 2768.0782419707252\n",
      "Epoch 760, Loss: 2764.6918371507713\n",
      "Epoch 770, Loss: 2761.315321297655\n",
      "Epoch 780, Loss: 2757.949683136735\n",
      "Epoch 790, Loss: 2754.5966605787166\n",
      "Epoch 800, Loss: 2751.253940135259\n",
      "Epoch 810, Loss: 2747.9200347718315\n",
      "Epoch 820, Loss: 2744.597275962904\n",
      "Epoch 830, Loss: 2741.285574373286\n",
      "Epoch 840, Loss: 2737.9860554405486\n",
      "Epoch 850, Loss: 2734.695136634521\n",
      "Epoch 860, Loss: 2731.412866946012\n",
      "Epoch 870, Loss: 2728.139250319209\n",
      "Epoch 880, Loss: 2724.8749510195667\n",
      "Epoch 890, Loss: 2721.6198667141052\n",
      "Epoch 900, Loss: 2718.3744556300717\n",
      "Epoch 910, Loss: 2715.1392654330284\n",
      "Epoch 920, Loss: 2711.912707987651\n",
      "Epoch 930, Loss: 2708.6945423603056\n",
      "Epoch 940, Loss: 2705.4846356399357\n",
      "Epoch 950, Loss: 2702.284271022193\n",
      "Epoch 960, Loss: 2699.0919812945276\n",
      "Epoch 970, Loss: 2695.9069998145105\n",
      "Epoch 980, Loss: 2692.729499704391\n",
      "Epoch 990, Loss: 2689.560882040486\n"
     ]
    }
   ],
   "source": [
    "# Generating a dataset of integer products\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(1, 21):\n",
    "    for j in range(1, 21):\n",
    "        inputs.append([i, j])\n",
    "        targets.append([i * j])\n",
    "\n",
    "# Convert the dataset to numpy arrays\n",
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)\n",
    "\n",
    "\n",
    "# Convert the dataset to PyTorch tensors\n",
    "inputs = torch.Tensor(inputs)\n",
    "targets = torch.Tensor(targets)\n",
    "\n",
    "# Create a model instance\n",
    "model = ACT_reservoir(2, 5, 1, learning_rate=0.0001, ponder_cost=0.05)\n",
    "\n",
    "# Train the model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xi, target in zip(inputs, targets):\n",
    "        loss = model.train(xi.numpy(), target.numpy())\n",
    "        total_loss += loss\n",
    "    \n",
    "    if epoch % 10 == 0:  # Print every 100 epochs\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss/len(inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted product of 15 and 5: 561333.625\n",
      "N_steps: 1\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.Tensor([100000, 10])\n",
    "predicted_output, steps = model.predict(sample_input.numpy())\n",
    "predicted_product = predicted_output.item()\n",
    "print(f\"Predicted product of 15 and 5: {predicted_product}\")\n",
    "print(f\"N_steps: {steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
